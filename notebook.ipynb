{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available, skipping this model\")\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173e624",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ## Load the Datasets\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\n",
    "    '/home/taranarmo/ironhack/week7/nlp-project/dataset/training_data.csv',\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['label', 'text']\n",
    ")\n",
    "\n",
    "# Load testing data\n",
    "test_df = pd.read_csv(\n",
    "    '/home/taranarmo/ironhack/week7/nlp-project/dataset/testing_data.csv',\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "    names=['label', 'text']\n",
    ")\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Testing data shape:\", test_df.shape)\n",
    "\n",
    "# Display first few rows of training data\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTraining data info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nLabel distribution in training data:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nFirst 5 rows of testing data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nLabel distribution in testing data (should be all 2s initially):\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# ## Split the Training Data\n",
    "# Split the training data into training and validation sets\n",
    "X = train_df['text']\n",
    "y = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set shape: {X_test.shape[0]} samples\")\n",
    "print(f\"Training label distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Validation label distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Text Preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and special characters\n",
    "    - Removing extra whitespaces\n",
    "    - Removing stopwords\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training and validation sets\n",
    "print(\"\\nApplying preprocessing to training and validation sets...\")\n",
    "X_train_processed = X_train.apply(preprocess_text)\n",
    "X_test_processed = X_test.apply(preprocess_text)\n",
    "\n",
    "# Also preprocess the testing data\n",
    "X_test_final = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Example of original text: {X_train.iloc[0]}\")\n",
    "print(f\"Example of processed text: {X_train_processed.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Feature Extraction using TF-IDF\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,        # Limit to top 10k features\n",
    "    ngram_range=(1, 2),        # Use unigrams and bigrams\n",
    "    stop_words='english',      # Remove English stop words\n",
    "    lowercase=True,            # Already handled in preprocessing\n",
    "    min_df=2,                  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95                # Ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the training data and transform\n",
    "print(\"\\nApplying TF-IDF vectorization...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Transform the final test data\n",
    "X_test_final_tfidf = tfidf_vectorizer.transform(X_test_final)\n",
    "\n",
    "print(f\"Training TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation TF-IDF shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Final test TF-IDF shape: {X_test_final_tfidf.shape}\")\n",
    "\n",
    "# Also try with Count Vectorizer (Bag of Words)\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit the count vectorizer on the training data and transform\n",
    "print(\"\\nApplying Bag of Words vectorization...\")\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train_processed)\n",
    "X_test_bow = count_vectorizer.transform(X_test_processed)\n",
    "X_test_final_bow = count_vectorizer.transform(X_test_final)\n",
    "\n",
    "print(f\"Training BoW shape: {X_train_bow.shape}\")\n",
    "print(f\"Validation BoW shape: {X_test_bow.shape}\")\n",
    "print(f\"Final test BoW shape: {X_test_final_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model Training and Evaluation\n",
    "\n",
    "# Define models to try\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Multinomial Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Conditionally add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, n_estimators=100)\n",
    "    print(\"XGBoost model added to the models list\")\n",
    "else:\n",
    "    print(\"XGBoost not available, skipping this model\")\n",
    "\n",
    "# Train and evaluate each model using TF-IDF features\n",
    "results = {}\n",
    "\n",
    "print(\"\\nTraining and evaluating models with TF-IDF features...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]  # Probability of positive class\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Ensure models directory exists\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# Save all original models\n",
    "print(\"\\nSaving original models...\")\n",
    "for name, result in results.items():\n",
    "    model_filename = f\"models/original_{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(result['model'], model_filename)\n",
    "    print(f\"Saved {name} model to {model_filename}\")\n",
    "\n",
    "# Also evaluate some models with Bag of Words features\n",
    "print(\"\\nEvaluating selected models with Bag of Words features...\")\n",
    "\n",
    "bow_results = {}\n",
    "\n",
    "for name in ['Logistic Regression', 'Multinomial Naive Bayes']:\n",
    "    if name in models:\n",
    "        model = models[name]\n",
    "        print(f\"\\nTraining {name} with BoW features...\")\n",
    "\n",
    "        # Train on BoW features\n",
    "        model_bow = type(model)(**model.get_params())  # Create a new instance with same parameters\n",
    "        model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_bow = model_bow.predict(X_test_bow)\n",
    "        y_pred_proba_bow = model_bow.predict_proba(X_test_bow)[:, 1]\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "        auc_score_bow = roc_auc_score(y_test, y_pred_proba_bow)\n",
    "\n",
    "        # Store results\n",
    "        bow_results[name] = {\n",
    "            'model': model_bow,\n",
    "            'accuracy': accuracy_bow,\n",
    "            'auc_score': auc_score_bow,\n",
    "            'predictions': y_pred_bow,\n",
    "            'probabilities': y_pred_proba_bow\n",
    "        }\n",
    "\n",
    "        print(f\"{name} (BoW) - Accuracy: {accuracy_bow:.4f}, AUC: {auc_score_bow:.4f}\")\n",
    "\n",
    "# Save Bag of Words models\n",
    "print(\"\\nSaving Bag of Words models...\")\n",
    "for name, result in bow_results.items():\n",
    "    model_filename = f\"models/bow_{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(result['model'], model_filename)\n",
    "    print(f\"Saved {name} (BoW) model to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd698c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameter Tuning\n",
    "\n",
    "print(\"\\nPerforming hyperparameter tuning for best performing models...\")\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']  # Required for l1 penalty\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Multinomial Naive Bayes': {\n",
    "        'alpha': [0.1, 1.0, 10.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add XGBoost to the grid if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "tuned_results = {}\n",
    "\n",
    "for name, params in param_grids.items():\n",
    "    print(f\"\\nTuning hyperparameters for {name}...\")\n",
    "\n",
    "    # Create a new instance of the model with standard parameters\n",
    "    if name == 'Logistic Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif name == 'Random Forest':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    elif name == 'SVM':\n",
    "        model = SVC(random_state=42, probability=True)\n",
    "    elif name == 'Multinomial Naive Bayes':\n",
    "        model = MultinomialNB()\n",
    "    elif name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
    "        model = xgb.XGBClassifier(random_state=42)\n",
    "    else:\n",
    "        continue  # Skip if model not available\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=params,\n",
    "        cv=3,  # 3-fold cross-validation to save time\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search on the training data\n",
    "    grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on validation set\n",
    "    y_pred_tuned = best_model.predict(X_test_tfidf)\n",
    "    y_pred_proba_tuned = best_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    auc_score_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "\n",
    "    # Store results\n",
    "    tuned_results[name] = {\n",
    "        'model': best_model,\n",
    "        'accuracy': accuracy_tuned,\n",
    "        'auc_score': auc_score_tuned,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'predictions': y_pred_tuned,\n",
    "        'probabilities': y_pred_proba_tuned\n",
    "    }\n",
    "\n",
    "    print(f\"{name} (Tuned) - Accuracy: {accuracy_tuned:.4f}, AUC: {auc_score_tuned:.4f}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Save all tuned models\n",
    "print(\"\\nSaving tuned models...\")\n",
    "for name, result in tuned_results.items():\n",
    "    model_filename = f\"models/tuned_{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(result['model'], model_filename)\n",
    "    print(f\"Saved {name} (Tuned) model to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67685dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model Comparison\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for name, result in tuned_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'AUC Score': result['auc_score'],\n",
    "        'Type': 'Tuned',\n",
    "        'Best_Params': str(result.get('best_params', 'N/A'))\n",
    "    })\n",
    "\n",
    "for name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'AUC Score': result['auc_score'],\n",
    "        'Type': 'Original',\n",
    "        'Best_Params': 'N/A'\n",
    "    })\n",
    "\n",
    "# Also add Bag of Words results to comparison\n",
    "for name, result in bow_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': f\"{name} (BoW)\",\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'AUC Score': result['auc_score'],\n",
    "        'Type': 'Bag of Words',\n",
    "        'Best_Params': 'N/A'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Save comparison table to CSV\n",
    "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\nModel comparison results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Create more detailed comparison tables\n",
    "original_vs_tuned_comparison = []\n",
    "for name in models.keys():\n",
    "    if name in results and name in tuned_results:\n",
    "        original_result = results[name]\n",
    "        tuned_result = tuned_results[name]\n",
    "        original_vs_tuned_comparison.append({\n",
    "            'Model': name,\n",
    "            'Original_Accuracy': original_result['accuracy'],\n",
    "            'Tuned_Accuracy': tuned_result['accuracy'],\n",
    "            'Original_AUC': original_result['auc_score'],\n",
    "            'Tuned_AUC': tuned_result['auc_score'],\n",
    "            'Accuracy_Improvement': tuned_result['accuracy'] - original_result['accuracy'],\n",
    "            'AUC_Improvement': tuned_result['auc_score'] - original_result['auc_score'],\n",
    "            'Best_Params': str(tuned_result.get('best_params', 'N/A'))\n",
    "        })\n",
    "\n",
    "if original_vs_tuned_comparison:\n",
    "    comparison_detail_df = pd.DataFrame(original_vs_tuned_comparison)\n",
    "    print(\"\\nDetailed Model Comparison (Original vs Tuned):\")\n",
    "    print(comparison_detail_df)\n",
    "\n",
    "    # Save detailed comparison to CSV\n",
    "    comparison_detail_df.to_csv('detailed_model_comparison.csv', index=False)\n",
    "    print(\"\\nDetailed model comparison results saved to 'detailed_model_comparison.csv'\")\n",
    "\n",
    "# Find the best model based on AUC score\n",
    "best_model_name = comparison_df.loc[comparison_df['AUC Score'].idxmax()]['Model']\n",
    "best_model_type = comparison_df.loc[comparison_df['AUC Score'].idxmax()]['Type']\n",
    "best_auc = comparison_df['AUC Score'].max()\n",
    "best_accuracy = comparison_df.loc[comparison_df['AUC Score'].idxmax()]['Accuracy']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} ({best_model_type})\")\n",
    "print(f\"Best AUC Score: {best_auc:.4f}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Get the actual best model\n",
    "if best_model_type == 'Tuned':\n",
    "    best_model = tuned_results[best_model_name]['model']\n",
    "else:\n",
    "    best_model = results[best_model_name]['model']\n",
    "\n",
    "# Detailed classification report for the best model\n",
    "print(f\"\\nDetailed Classification Report for {best_model_name} ({best_model_type}):\")\n",
    "if best_model_type == 'Tuned':\n",
    "    best_predictions = tuned_results[best_model_name]['predictions']\n",
    "else:\n",
    "    best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Create model performance comparison visualizations\n",
    "print(\"\\nCreating model performance comparison visualizations...\")\n",
    "\n",
    "# Create a bar plot for accuracy comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df_sorted = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "bars1 = plt.barh(range(len(comparison_df_sorted)), comparison_df_sorted['Accuracy'],\n",
    "                 color=['red' if 'Original' in t else 'blue' if 'Tuned' in t else 'green'\n",
    "                        for t in comparison_df_sorted['Type']])\n",
    "plt.yticks(range(len(comparison_df_sorted)), comparison_df_sorted['Model'])\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "for i, v in enumerate(comparison_df_sorted['Accuracy']):\n",
    "    plt.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Create a bar plot for AUC comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "bars2 = plt.barh(range(len(comparison_df_sorted)), comparison_df_sorted['AUC Score'],\n",
    "                 color=['red' if 'Original' in t else 'blue' if 'Tuned' in t else 'green'\n",
    "                        for t in comparison_df_sorted['Type']])\n",
    "plt.yticks(range(len(comparison_df_sorted)), comparison_df_sorted['Model'])\n",
    "plt.xlabel('AUC Score')\n",
    "plt.title('Model AUC Score Comparison')\n",
    "for i, v in enumerate(comparison_df_sorted['AUC Score']):\n",
    "    plt.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Model performance comparison plot saved as 'visualizations/model_performance_comparison.png'\")\n",
    "\n",
    "# Create a detailed comparison plot for original vs tuned models\n",
    "if 'comparison_detail_df' in locals():\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Plot original vs tuned accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    x = np.arange(len(comparison_detail_df))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, comparison_detail_df['Original_Accuracy'], width, label='Original', alpha=0.8)\n",
    "    plt.bar(x + width/2, comparison_detail_df['Tuned_Accuracy'], width, label='Tuned', alpha=0.8)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Original vs Tuned Model Accuracy')\n",
    "    plt.xticks(x, comparison_detail_df['Model'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Plot original vs tuned AUC\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(x - width/2, comparison_detail_df['Original_AUC'], width, label='Original', alpha=0.8)\n",
    "    plt.bar(x + width/2, comparison_detail_df['Tuned_AUC'], width, label='Tuned', alpha=0.8)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Original vs Tuned Model AUC')\n",
    "    plt.xticks(x, comparison_detail_df['Model'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Plot accuracy improvement\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(x, comparison_detail_df['Accuracy_Improvement'],\n",
    "            color=['green' if imp > 0 else 'red' for imp in comparison_detail_df['Accuracy_Improvement']])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy Improvement')\n",
    "    plt.title('Accuracy Improvement from Tuning')\n",
    "    plt.xticks(x, comparison_detail_df['Model'], rotation=45, ha='right')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Plot AUC improvement\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(x, comparison_detail_df['AUC_Improvement'],\n",
    "            color=['green' if imp > 0 else 'red' for imp in comparison_detail_df['AUC_Improvement']])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('AUC Improvement')\n",
    "    plt.title('AUC Improvement from Tuning')\n",
    "    plt.xticks(x, comparison_detail_df['Model'], rotation=45, ha='right')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/original_vs_tuned_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Original vs Tuned comparison plot saved as 'visualizations/original_vs_tuned_comparison.png'\")\n",
    "\n",
    "# Confusion Matrix\n",
    "try:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, best_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real News', 'Fake News'],\n",
    "                yticklabels=['Real News', 'Fake News'])\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot instead of showing\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    print(\"Confusion matrix plot saved as 'confusion_matrix.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create confusion matrix plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ef23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Final Predictions on Testing Data\n",
    "\n",
    "print(f\"\\nUsing the best model ({best_model_name}) to predict labels for testing data...\")\n",
    "\n",
    "# Transform the testing data using the same TF-IDF vectorizer used for training\n",
    "X_test_final_transformed = tfidf_vectorizer.transform(X_test_final)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "final_predictions = best_model.predict(X_test_final_transformed)\n",
    "\n",
    "print(f\"Predicted labels shape: {final_predictions.shape}\")\n",
    "print(f\"Predicted label distribution:\\n{pd.Series(final_predictions).value_counts()}\")\n",
    "\n",
    "# Create a copy of the test dataframe to update\n",
    "test_predictions_df = test_df.copy()\n",
    "\n",
    "# Replace the labels (currently all 2s) with our predictions\n",
    "test_predictions_df['label'] = final_predictions\n",
    "\n",
    "print(f\"\\nUpdated testing data shape: {test_predictions_df.shape}\")\n",
    "print(\"First 10 rows of updated testing data:\")\n",
    "print(test_predictions_df.head(10))\n",
    "\n",
    "# Save the updated testing data with predictions\n",
    "output_file_path = 'testing_data_with_predictions.csv'\n",
    "test_predictions_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "print(f\"\\nPredictions saved to: {output_file_path}\")\n",
    "\n",
    "print(\"\\nFake News Classification Project completed!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best AUC score: {best_auc:.4f}\")\n",
    "print(f\"Predictions made on testing data and saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
